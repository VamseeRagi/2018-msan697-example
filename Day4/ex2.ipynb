{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toDoubleSafe(v):\n",
    "    try:\n",
    "        return float(v)\n",
    "    except ValueError:\n",
    "        return str(v) #if it is not a float type return as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load and convert the data\n",
    "census_raw = sc.textFile(\"../Data/adult.raw\", 4).map(lambda x:  x.split(\", \"))\n",
    "census_raw = census_raw.map(lambda row:  [toDoubleSafe(x) for x in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "adultschema = StructType([\n",
    "    StructField(\"age\",DoubleType(),True),\n",
    "    StructField(\"capital_gain\",DoubleType(),True),\n",
    "    StructField(\"capital_loss\",DoubleType(),True),\n",
    "    StructField(\"education\",StringType(),True),\n",
    "    StructField(\"fnlwgt\",DoubleType(),True),\n",
    "    StructField(\"hours_per_week\",DoubleType(),True),\n",
    "    StructField(\"income\",StringType(),True),\n",
    "    StructField(\"marital_status\",StringType(),True),\n",
    "    StructField(\"native_country\",StringType(),True),\n",
    "    StructField(\"occupation\",StringType(),True),\n",
    "    StructField(\"race\",StringType(),True),\n",
    "    StructField(\"relationship\",StringType(),True),\n",
    "    StructField(\"sex\",StringType(),True),\n",
    "    StructField(\"workclass\",StringType(),True),\n",
    "])\n",
    "\n",
    "# Create a dataframe.\n",
    "from pyspark.sql import Row\n",
    "columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"marital_status\",\n",
    "           \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\",\n",
    "           \"hours_per_week\", \"native_country\", \"income\"]\n",
    "dfraw = sqlContext.createDataFrame(census_raw.map(lambda row: Row(**{x[0]: x[1] for x in zip(columns, row)})), \\\n",
    "                                    adultschema)\n",
    "#dfraw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39.0, 'State-gov', 77516.0, 'Bachelors', 'Never-married', 'Adm-clerical', 'Not-in-family', 'White', 'Male', 2174.0, 0.0, 40.0, 'United-States', '<=50K']]\n",
      "[[('age', 39.0), ('workclass', 'State-gov'), ('fnlwgt', 77516.0), ('education', 'Bachelors'), ('marital_status', 'Never-married'), ('occupation', 'Adm-clerical'), ('relationship', 'Not-in-family'), ('race', 'White'), ('sex', 'Male'), ('capital_gain', 2174.0), ('capital_loss', 0.0), ('hours_per_week', 40.0), ('native_country', 'United-States'), ('income', '<=50K')]]\n",
      "[{'hours_per_week': 40.0, 'workclass': 'State-gov', 'relationship': 'Not-in-family', 'age': 39.0, 'marital_status': 'Never-married', 'sex': 'Male', 'race': 'White', 'income': '<=50K', 'native_country': 'United-States', 'capital_loss': 0.0, 'education': 'Bachelors', 'fnlwgt': 77516.0, 'capital_gain': 2174.0, 'occupation': 'Adm-clerical'}]\n",
      "[Row(age=39.0, capital_gain=2174.0, capital_loss=0.0, education='Bachelors', fnlwgt=77516.0, hours_per_week=40.0, income='<=50K', marital_status='Never-married', native_country='United-States', occupation='Adm-clerical', race='White', relationship='Not-in-family', sex='Male', workclass='State-gov')]\n"
     ]
    }
   ],
   "source": [
    "# Original:\n",
    "print census_raw.take(1)\n",
    "\n",
    "# Returns a list of tuples.\n",
    "# zip() : returns a list of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables.\n",
    "print census_raw.map(lambda x :  zip(columns, x)).take(1) \n",
    "\n",
    "# Transform a list into a list with keyword arguments.\n",
    "print census_raw.map(lambda x : {x[0]: x[1] for x in zip(columns, x)}).take(1) \n",
    "\n",
    "# Transform into a row using variable with keywords.\n",
    "# As this is keyworded, createDataFrame() will match the column name and apply the defined schema.\n",
    "print census_raw.map(lambda x : Row(**{x[0]: x[1] for x in zip(columns, x)})).take(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|       workclass|count|\n",
      "+----------------+-----+\n",
      "|         Private|33906|\n",
      "|Self-emp-not-inc| 3862|\n",
      "|       Local-gov| 3136|\n",
      "|               ?| 2799|\n",
      "|       State-gov| 1981|\n",
      "|    Self-emp-inc| 1695|\n",
      "|     Federal-gov| 1432|\n",
      "|     Without-pay|   21|\n",
      "|    Never-worked|   10|\n",
      "+----------------+-----+\n",
      "\n",
      "None\n",
      "+-----------------+-----+\n",
      "|       occupation|count|\n",
      "+-----------------+-----+\n",
      "|   Prof-specialty| 6172|\n",
      "|     Craft-repair| 6112|\n",
      "|  Exec-managerial| 6086|\n",
      "|     Adm-clerical| 5611|\n",
      "|            Sales| 5504|\n",
      "|    Other-service| 4923|\n",
      "|Machine-op-inspct| 3022|\n",
      "|                ?| 2809|\n",
      "| Transport-moving| 2355|\n",
      "|Handlers-cleaners| 2072|\n",
      "|  Farming-fishing| 1490|\n",
      "|     Tech-support| 1446|\n",
      "|  Protective-serv|  983|\n",
      "|  Priv-house-serv|  242|\n",
      "|     Armed-Forces|   15|\n",
      "+-----------------+-----+\n",
      "\n",
      "None\n",
      "+------------------+-----+\n",
      "|    native_country|count|\n",
      "+------------------+-----+\n",
      "|     United-States|43832|\n",
      "|            Mexico|  951|\n",
      "|                 ?|  857|\n",
      "|       Philippines|  295|\n",
      "|           Germany|  206|\n",
      "|       Puerto-Rico|  184|\n",
      "|            Canada|  182|\n",
      "|       El-Salvador|  155|\n",
      "|             India|  151|\n",
      "|              Cuba|  138|\n",
      "|           England|  127|\n",
      "|             China|  122|\n",
      "|             South|  115|\n",
      "|           Jamaica|  106|\n",
      "|             Italy|  105|\n",
      "|Dominican-Republic|  103|\n",
      "|             Japan|   92|\n",
      "|         Guatemala|   88|\n",
      "|            Poland|   87|\n",
      "|           Vietnam|   86|\n",
      "+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Check the most commonly used vals.\n",
    "print dfraw.groupBy(dfraw[\"workclass\"]).count().orderBy(\"count\",ascending=False).show()\n",
    "print dfraw.groupBy(dfraw[\"occupation\"]).count().orderBy(\"count\",ascending=False).show()\n",
    "print dfraw.groupBy(dfraw[\"native_country\"]).count().orderBy(\"count\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Missing data imputation - Impute the most common row for \"?\".\n",
    "dfrawrp = dfraw.na.replace([\"?\"], [\"Private\"], [\"workclass\"])\n",
    "dfrawrpl = dfrawrp.na.replace([\"?\"], [\"Prof-specialty\"], [\"occupation\"])\n",
    "dfrawnona = dfrawrpl.na.replace([\"?\"], [\"United-States\"], [\"native_country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------+------------+--------+--------------+------+--------------------+--------------+-----------------+------------------+-------------+------+----------------+\n",
      "| age|capital_gain|capital_loss|   education|  fnlwgt|hours_per_week|income|      marital_status|native_country|       occupation|              race| relationship|   sex|       workclass|\n",
      "+----+------------+------------+------------+--------+--------------+------+--------------------+--------------+-----------------+------------------+-------------+------+----------------+\n",
      "|39.0|      2174.0|         0.0|   Bachelors| 77516.0|          40.0| <=50K|       Never-married| United-States|     Adm-clerical|             White|Not-in-family|  Male|       State-gov|\n",
      "|50.0|         0.0|         0.0|   Bachelors| 83311.0|          13.0| <=50K|  Married-civ-spouse| United-States|  Exec-managerial|             White|      Husband|  Male|Self-emp-not-inc|\n",
      "|38.0|         0.0|         0.0|     HS-grad|215646.0|          40.0| <=50K|            Divorced| United-States|Handlers-cleaners|             White|Not-in-family|  Male|         Private|\n",
      "|53.0|         0.0|         0.0|        11th|234721.0|          40.0| <=50K|  Married-civ-spouse| United-States|Handlers-cleaners|             Black|      Husband|  Male|         Private|\n",
      "|28.0|         0.0|         0.0|   Bachelors|338409.0|          40.0| <=50K|  Married-civ-spouse|          Cuba|   Prof-specialty|             Black|         Wife|Female|         Private|\n",
      "|37.0|         0.0|         0.0|     Masters|284582.0|          40.0| <=50K|  Married-civ-spouse| United-States|  Exec-managerial|             White|         Wife|Female|         Private|\n",
      "|49.0|         0.0|         0.0|         9th|160187.0|          16.0| <=50K|Married-spouse-ab...|       Jamaica|    Other-service|             Black|Not-in-family|Female|         Private|\n",
      "|52.0|         0.0|         0.0|     HS-grad|209642.0|          45.0|  >50K|  Married-civ-spouse| United-States|  Exec-managerial|             White|      Husband|  Male|Self-emp-not-inc|\n",
      "|31.0|     14084.0|         0.0|     Masters| 45781.0|          50.0|  >50K|       Never-married| United-States|   Prof-specialty|             White|Not-in-family|Female|         Private|\n",
      "|42.0|      5178.0|         0.0|   Bachelors|159449.0|          40.0|  >50K|  Married-civ-spouse| United-States|  Exec-managerial|             White|      Husband|  Male|         Private|\n",
      "|37.0|         0.0|         0.0|Some-college|280464.0|          80.0|  >50K|  Married-civ-spouse| United-States|  Exec-managerial|             Black|      Husband|  Male|         Private|\n",
      "|30.0|         0.0|         0.0|   Bachelors|141297.0|          40.0|  >50K|  Married-civ-spouse|         India|   Prof-specialty|Asian-Pac-Islander|      Husband|  Male|       State-gov|\n",
      "|23.0|         0.0|         0.0|   Bachelors|122272.0|          30.0| <=50K|       Never-married| United-States|     Adm-clerical|             White|    Own-child|Female|         Private|\n",
      "|32.0|         0.0|         0.0|  Assoc-acdm|205019.0|          50.0| <=50K|       Never-married| United-States|            Sales|             Black|Not-in-family|  Male|         Private|\n",
      "|40.0|         0.0|         0.0|   Assoc-voc|121772.0|          40.0|  >50K|  Married-civ-spouse| United-States|     Craft-repair|Asian-Pac-Islander|      Husband|  Male|         Private|\n",
      "|34.0|         0.0|         0.0|     7th-8th|245487.0|          45.0| <=50K|  Married-civ-spouse|        Mexico| Transport-moving|Amer-Indian-Eskimo|      Husband|  Male|         Private|\n",
      "|25.0|         0.0|         0.0|     HS-grad|176756.0|          35.0| <=50K|       Never-married| United-States|  Farming-fishing|             White|    Own-child|  Male|Self-emp-not-inc|\n",
      "|32.0|         0.0|         0.0|     HS-grad|186824.0|          40.0| <=50K|       Never-married| United-States|Machine-op-inspct|             White|    Unmarried|  Male|         Private|\n",
      "|38.0|         0.0|         0.0|        11th| 28887.0|          50.0| <=50K|  Married-civ-spouse| United-States|            Sales|             White|      Husband|  Male|         Private|\n",
      "|43.0|         0.0|         0.0|     Masters|292175.0|          45.0|  >50K|            Divorced| United-States|  Exec-managerial|             White|    Unmarried|Female|Self-emp-not-inc|\n",
      "+----+------------+------------+------------+--------+--------------+------+--------------------+--------------+-----------------+------------------+-------------+------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfrawnona.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#converting strings to numeric values\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def indexStringColumns(df, cols):\n",
    "    #variable newdf will be updated several times\n",
    "    newdf = df\n",
    "    \n",
    "    for c in cols:\n",
    "        #For each given colum, fits StringIndexerModel.\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-num\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-num\" suffix. \n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf\n",
    "\n",
    "dfnumeric = indexStringColumns(dfrawnona, [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\", \"income\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------+--------+--------------+---------+---------+--------------+----------+------------+----+---+--------------+------+\n",
      "| age|capital_gain|capital_loss|  fnlwgt|hours_per_week|workclass|education|marital_status|occupation|relationship|race|sex|native_country|income|\n",
      "+----+------------+------------+--------+--------------+---------+---------+--------------+----------+------------+----+---+--------------+------+\n",
      "|39.0|      2174.0|         0.0| 77516.0|          40.0|      3.0|      2.0|           1.0|       3.0|         1.0| 0.0|0.0|           0.0|   0.0|\n",
      "|50.0|         0.0|         0.0| 83311.0|          13.0|      1.0|      2.0|           0.0|       2.0|         0.0| 0.0|0.0|           0.0|   0.0|\n",
      "|38.0|         0.0|         0.0|215646.0|          40.0|      0.0|      0.0|           2.0|       8.0|         1.0| 0.0|0.0|           0.0|   0.0|\n",
      "|53.0|         0.0|         0.0|234721.0|          40.0|      0.0|      5.0|           0.0|       8.0|         0.0| 1.0|0.0|           0.0|   0.0|\n",
      "|28.0|         0.0|         0.0|338409.0|          40.0|      0.0|      2.0|           0.0|       0.0|         4.0| 1.0|1.0|           8.0|   0.0|\n",
      "|37.0|         0.0|         0.0|284582.0|          40.0|      0.0|      3.0|           0.0|       2.0|         4.0| 0.0|1.0|           0.0|   0.0|\n",
      "|49.0|         0.0|         0.0|160187.0|          16.0|      0.0|     10.0|           5.0|       5.0|         1.0| 1.0|1.0|          12.0|   0.0|\n",
      "|52.0|         0.0|         0.0|209642.0|          45.0|      1.0|      0.0|           0.0|       2.0|         0.0| 0.0|0.0|           0.0|   1.0|\n",
      "|31.0|     14084.0|         0.0| 45781.0|          50.0|      0.0|      3.0|           1.0|       0.0|         1.0| 0.0|1.0|           0.0|   1.0|\n",
      "|42.0|      5178.0|         0.0|159449.0|          40.0|      0.0|      2.0|           0.0|       2.0|         0.0| 0.0|0.0|           0.0|   1.0|\n",
      "|37.0|         0.0|         0.0|280464.0|          80.0|      0.0|      1.0|           0.0|       2.0|         0.0| 1.0|0.0|           0.0|   1.0|\n",
      "|30.0|         0.0|         0.0|141297.0|          40.0|      3.0|      2.0|           0.0|       0.0|         0.0| 2.0|0.0|           7.0|   1.0|\n",
      "|23.0|         0.0|         0.0|122272.0|          30.0|      0.0|      2.0|           1.0|       3.0|         2.0| 0.0|1.0|           0.0|   0.0|\n",
      "|32.0|         0.0|         0.0|205019.0|          50.0|      0.0|      6.0|           1.0|       4.0|         1.0| 1.0|0.0|           0.0|   0.0|\n",
      "|40.0|         0.0|         0.0|121772.0|          40.0|      0.0|      4.0|           0.0|       1.0|         0.0| 2.0|0.0|           0.0|   1.0|\n",
      "|34.0|         0.0|         0.0|245487.0|          45.0|      0.0|      8.0|           0.0|       7.0|         0.0| 3.0|0.0|           1.0|   0.0|\n",
      "|25.0|         0.0|         0.0|176756.0|          35.0|      1.0|      0.0|           1.0|       9.0|         2.0| 0.0|0.0|           0.0|   0.0|\n",
      "|32.0|         0.0|         0.0|186824.0|          40.0|      0.0|      0.0|           1.0|       6.0|         3.0| 0.0|0.0|           0.0|   0.0|\n",
      "|38.0|         0.0|         0.0| 28887.0|          50.0|      0.0|      5.0|           0.0|       4.0|         0.0| 0.0|0.0|           0.0|   0.0|\n",
      "|43.0|         0.0|         0.0|292175.0|          45.0|      1.0|      3.0|           2.0|       2.0|         3.0| 0.0|1.0|           0.0|   1.0|\n",
      "+----+------------+------------+--------+--------------+---------+---------+--------------+----------+------------+----+---+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfnumeric.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "def oneHotEncodeColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        #For each given colum, create OneHotEncoder. \n",
    "        #dropLast : Whether to drop the last category in the encoded vector (default: true)\n",
    "        onehotenc = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-onehot\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-onehot\" suffix. \n",
    "        newdf = onehotenc.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf\n",
    "\n",
    "dfhot = oneHotEncodeColumns(dfnumeric, [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"native_country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------+--------+--------------+---+------+-------------+---------------+--------------+--------------+-------------+-------------+---------------+\n",
      "| age|capital_gain|capital_loss|  fnlwgt|hours_per_week|sex|income|    workclass|      education|marital_status|    occupation| relationship|         race| native_country|\n",
      "+----+------------+------------+--------+--------------+---+------+-------------+---------------+--------------+--------------+-------------+-------------+---------------+\n",
      "|39.0|      2174.0|         0.0| 77516.0|          40.0|0.0|   0.0|(8,[3],[1.0])| (16,[2],[1.0])| (7,[1],[1.0])|(14,[3],[1.0])|(6,[1],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|50.0|         0.0|         0.0| 83311.0|          13.0|0.0|   0.0|(8,[1],[1.0])| (16,[2],[1.0])| (7,[0],[1.0])|(14,[2],[1.0])|(6,[0],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|38.0|         0.0|         0.0|215646.0|          40.0|0.0|   0.0|(8,[0],[1.0])| (16,[0],[1.0])| (7,[2],[1.0])|(14,[8],[1.0])|(6,[1],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|53.0|         0.0|         0.0|234721.0|          40.0|0.0|   0.0|(8,[0],[1.0])| (16,[5],[1.0])| (7,[0],[1.0])|(14,[8],[1.0])|(6,[0],[1.0])|(5,[1],[1.0])| (41,[0],[1.0])|\n",
      "|28.0|         0.0|         0.0|338409.0|          40.0|1.0|   0.0|(8,[0],[1.0])| (16,[2],[1.0])| (7,[0],[1.0])|(14,[0],[1.0])|(6,[4],[1.0])|(5,[1],[1.0])| (41,[8],[1.0])|\n",
      "|37.0|         0.0|         0.0|284582.0|          40.0|1.0|   0.0|(8,[0],[1.0])| (16,[3],[1.0])| (7,[0],[1.0])|(14,[2],[1.0])|(6,[4],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|49.0|         0.0|         0.0|160187.0|          16.0|1.0|   0.0|(8,[0],[1.0])|(16,[10],[1.0])| (7,[5],[1.0])|(14,[5],[1.0])|(6,[1],[1.0])|(5,[1],[1.0])|(41,[12],[1.0])|\n",
      "|52.0|         0.0|         0.0|209642.0|          45.0|0.0|   1.0|(8,[1],[1.0])| (16,[0],[1.0])| (7,[0],[1.0])|(14,[2],[1.0])|(6,[0],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|31.0|     14084.0|         0.0| 45781.0|          50.0|1.0|   1.0|(8,[0],[1.0])| (16,[3],[1.0])| (7,[1],[1.0])|(14,[0],[1.0])|(6,[1],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|42.0|      5178.0|         0.0|159449.0|          40.0|0.0|   1.0|(8,[0],[1.0])| (16,[2],[1.0])| (7,[0],[1.0])|(14,[2],[1.0])|(6,[0],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|37.0|         0.0|         0.0|280464.0|          80.0|0.0|   1.0|(8,[0],[1.0])| (16,[1],[1.0])| (7,[0],[1.0])|(14,[2],[1.0])|(6,[0],[1.0])|(5,[1],[1.0])| (41,[0],[1.0])|\n",
      "|30.0|         0.0|         0.0|141297.0|          40.0|0.0|   1.0|(8,[3],[1.0])| (16,[2],[1.0])| (7,[0],[1.0])|(14,[0],[1.0])|(6,[0],[1.0])|(5,[2],[1.0])| (41,[7],[1.0])|\n",
      "|23.0|         0.0|         0.0|122272.0|          30.0|1.0|   0.0|(8,[0],[1.0])| (16,[2],[1.0])| (7,[1],[1.0])|(14,[3],[1.0])|(6,[2],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|32.0|         0.0|         0.0|205019.0|          50.0|0.0|   0.0|(8,[0],[1.0])| (16,[6],[1.0])| (7,[1],[1.0])|(14,[4],[1.0])|(6,[1],[1.0])|(5,[1],[1.0])| (41,[0],[1.0])|\n",
      "|40.0|         0.0|         0.0|121772.0|          40.0|0.0|   1.0|(8,[0],[1.0])| (16,[4],[1.0])| (7,[0],[1.0])|(14,[1],[1.0])|(6,[0],[1.0])|(5,[2],[1.0])| (41,[0],[1.0])|\n",
      "|34.0|         0.0|         0.0|245487.0|          45.0|0.0|   0.0|(8,[0],[1.0])| (16,[8],[1.0])| (7,[0],[1.0])|(14,[7],[1.0])|(6,[0],[1.0])|(5,[3],[1.0])| (41,[1],[1.0])|\n",
      "|25.0|         0.0|         0.0|176756.0|          35.0|0.0|   0.0|(8,[1],[1.0])| (16,[0],[1.0])| (7,[1],[1.0])|(14,[9],[1.0])|(6,[2],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|32.0|         0.0|         0.0|186824.0|          40.0|0.0|   0.0|(8,[0],[1.0])| (16,[0],[1.0])| (7,[1],[1.0])|(14,[6],[1.0])|(6,[3],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|38.0|         0.0|         0.0| 28887.0|          50.0|0.0|   0.0|(8,[0],[1.0])| (16,[5],[1.0])| (7,[0],[1.0])|(14,[4],[1.0])|(6,[0],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|43.0|         0.0|         0.0|292175.0|          45.0|1.0|   1.0|(8,[1],[1.0])| (16,[3],[1.0])| (7,[2],[1.0])|(14,[2],[1.0])|(6,[3],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "+----+------------+------------+--------+--------------+---+------+-------------+---------------+--------------+--------------+-------------+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfhot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merging the data with Vector Assembler.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "input_cols=[\"age\",\"capital_gain\",\"capital_loss\",\"fnlwgt\",\"hours_per_week\",\"sex\",\"workclass\",\"education\",\"marital_status\",\"occupation\",\"relationship\",\"native_country\",\"race\"]\n",
    "\n",
    "#VectorAssembler takes a number of collumn names(inputCols) and output column name (outputCol)\n",
    "#and transforms a DataFrame to assemble the values in inputCols into one single vector with outputCol.\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=input_cols)\n",
    "#lpoints - labeled data.\n",
    "lpoints = va.transform(dfhot).select(\"features\", \"income\").withColumnRenamed(\"income\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(103,[0,1,3,4,9,1...|  0.0|\n",
      "|(103,[0,3,4,7,16,...|  0.0|\n",
      "|(103,[0,3,4,6,14,...|  0.0|\n",
      "|(103,[0,3,4,6,19,...|  0.0|\n",
      "|(103,[0,3,4,5,6,1...|  0.0|\n",
      "|(103,[0,3,4,5,6,1...|  0.0|\n",
      "|(103,[0,3,4,5,6,2...|  0.0|\n",
      "|(103,[0,3,4,7,14,...|  1.0|\n",
      "|(103,[0,1,3,4,5,6...|  1.0|\n",
      "|(103,[0,1,3,4,6,1...|  1.0|\n",
      "|(103,[0,3,4,6,15,...|  1.0|\n",
      "|(103,[0,3,4,9,16,...|  1.0|\n",
      "|(103,[0,3,4,5,6,1...|  0.0|\n",
      "|(103,[0,3,4,6,20,...|  0.0|\n",
      "|(103,[0,3,4,6,18,...|  1.0|\n",
      "|(103,[0,3,4,6,22,...|  0.0|\n",
      "|(103,[0,3,4,7,14,...|  0.0|\n",
      "|(103,[0,3,4,6,14,...|  0.0|\n",
      "|(103,[0,3,4,6,19,...|  0.0|\n",
      "|(103,[0,3,4,5,7,1...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lpoints.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Divide the dataset into training and testing sets.\n",
    "splits = lpoints.randomSplit([0.8, 0.2])\n",
    "\n",
    "#cache() : the algorithm is interative and training and data sets are going to be reused many times.\n",
    "adulttrain = splits[0].cache()\n",
    "adultvalid = splits[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train the model.\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(regParam=0.01, maxIter=1000, fitIntercept=True)\n",
    "lrmodel = lr.fit(adulttrain)\n",
    "#The above lines are same as..\n",
    "#lr = LogisticRegression()\n",
    "#lrmodel = lr.setParams(regParam=0.01, maxIter=1000, fitIntercept=True).fit(adulttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.020886655493,0.00013814208456,0.000536023519324,6.62226843118e-07,0.0270660441379,-0.487503141373,0.0161366388365,-0.383200902707,0.0907533856955,-0.122649521649,0.258675488878,0.579727153328,-0.724588401247,-1.42443086934,-0.327174695493,-0.0275767138075,0.752433780449,1.11409927583,0.112983115872,-1.03163157169,0.209985019504,-1.14798545423,-1.34006633173,1.68985368222,-1.25924296867,-0.666037687585,1.71649694015,-1.2636995093,-1.4126439157,-1.74917838824,0.829447909905,-0.671006009667,-0.309814152094,-0.325528322448,-0.29411582029,-0.230698769544,0.702149407656,0.188305041635,0.0412512600486,0.660236786877,-0.0307415709937,0.229514872575,-0.726889295886,-0.30092451067,-0.110126977672,-0.62334158471,-0.893698429616,0.426415766812,0.287298545489,-0.860518044844,0.38269314022,0.449933930274,-0.100124028741,-0.784646063433,-0.285406836653,1.27012603093,-0.529018656198,0.222965040344,-0.58954925667,0.31958228822,0.0517411473652,-0.431112372582,0.5186145274,-0.47000451594,0.000181497427842,0.240251716312,0.621371228847,-0.335089266049,-1.07348455207,0.166288657352,0.57204515456,-1.05222691792,0.0212102437023,-0.229313959005,-0.110171880815,-0.616622816827,-1.53034507943,-0.0200583926847,0.337297986515,0.0500949937364,0.399933621186,0.0206727650562,-1.14324168145,-0.890563050938,0.0415249672852,0.448451515351,0.826030657299,-0.534347253725,-0.495258125319,0.737645845117,-0.771822783478,1.08513721589,-0.97627352024,-0.614129894903,-0.729339436992,-0.111675269739,0.806390807656,-1.13895156714,0.0864894547376,-0.147660424703,0.182240377034,-0.350996231174,-0.0142671542003]\n",
      "-4.36736767225\n"
     ]
    }
   ],
   "source": [
    "#Interpret the model parameters\n",
    "print(lrmodel.coefficients)\n",
    "print(lrmodel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[0.66359415944655...|[0.66006730376248...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  1.0|[-0.1135414329086...|[0.47164509703669...|       1.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[0.94540936331228...|[0.72019102745677...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  1.0|[0.64857384994214...|[0.65668901109215...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  1.0|[-0.9479794955166...|[0.27929134323132...|       1.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[4.36950698924466...|[0.98750072994855...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[5.22655782204167...|[0.99465671918182...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[3.00146514442795...|[0.95264027328105...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  1.0|[1.01444144459535...|[0.73388844702834...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[2.26282117926111...|[0.90575074062506...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[2.96930708587819...|[0.95116810304098...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[0.52338225214549...|[0.62793830926269...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[0.15464569621968...|[0.53858455794218...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[3.48289098393129...|[0.97019702601913...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[2.43393582232502...|[0.91937874366454...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[5.67626172795300...|[0.99658535660501...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  1.0|[1.09652421739257...|[0.74960828229453...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[2.07471060515980...|[0.88842077553335...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  1.0|[-1.6605447538429...|[0.15968888377516...|       1.0|\n",
      "|(103,[0,1,3,4,5,6...|  1.0|[-2.3949824358000...|[0.08355611194003...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate models using test dataset.\n",
    "#First, transform the validation set.\n",
    "validpredicts = lrmodel.transform(adultvalid)\n",
    "validpredicts.show()\n",
    "\n",
    "#rawPrediction : includes two values - log-odds that a sample doesn't and does belong to the category (making > 50,000).\n",
    "#probability : the probability that the sample is not in the category.\n",
    "#prediction : proability that the sample belongs to the category.\n",
    "#validpredicts.select(\"rawPrediction\").collect()\n",
    "#validpredicts.select(\"probability\").collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC:0.905679568497\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model. default metric : Area Under ROC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "bceval = BinaryClassificationEvaluator()\n",
    "print (bceval.getMetricName() +\":\" + str(bceval.evaluate(validpredicts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderPR:0.755839393918\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model. metric : Area Under PR\n",
    "bceval.setMetricName(\"areaUnderPR\")\n",
    "print (bceval.getMetricName() +\":\" + str(bceval.evaluate(validpredicts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n-fold validation and the results.\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "cv = CrossValidator().setEstimator(lr).setEvaluator(bceval).setNumFolds(5)\n",
    "#ParamGridBuilder() â€“ combinations of parameters and their values.\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.maxIter, [1000]).addGrid(lr.regParam, [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]).build()\n",
    "#setEstimatorParamMaps() takes ParamGridBuilder().\n",
    "cv.setEstimatorParamMaps(paramGrid)\n",
    "cvmodel = cv.fit(adulttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0234338788762,0.000304796114907,0.00062874334433,7.77035489089e-07,0.0307924540079,-0.641099248989,-0.443318171871,-0.925069998667,-0.385154125631,-0.613372592103,-0.251827208678,0.158669624309,-1.26846300514,-4.89332618123,-0.581189343102,-0.24593770866,0.598810471799,0.987127931754,-0.111951081402,-1.52397636113,0.0128174398478,-1.61319972409,-1.82924200939,1.67144413992,-1.74256992422,-1.00488031153,1.69687112398,-1.760260614,-2.06164116827,-5.66628580183,1.2366685519,-1.43102072707,-1.02259537336,-1.02000653922,-1.01902169702,-0.970378937429,1.16944496017,-0.0479286537312,-0.140962876756,0.481171600081,-0.231439290576,0.0337140396313,-1.08113353137,-0.511945959385,-0.304846798908,-0.900248079746,-1.27130944447,0.255039899041,0.13218758793,-1.80910679783,0.276849741455,-0.389592730611,0.119805536582,-1.00326154074,-0.0370740551346,0.671326335818,-0.874230777888,-0.992978546689,-1.90048459587,-0.916423335565,-1.19798966162,-1.70286290138,-0.63243262278,-1.89392968124,-1.38553076516,-0.878322279168,-0.580649149758,-1.69570478025,-2.57077371045,-0.949339162182,-0.539295755822,-2.46638215641,-1.34814205279,-1.388533713,-1.31966801677,-2.06145097325,-3.25832539656,-1.1409095945,-0.694067511577,-1.24950857354,-0.747034102593,-1.28185083307,-2.68269255187,-2.19286381924,-1.16202105457,-0.774341377001,-0.320094419108,-1.94475639955,-1.83977330728,-0.459055686189,-2.12605130951,-0.0103375556944,-2.44726464872,-1.83301096915,-2.08407125132,-1.21725039163,-0.285559367163,-4.78121320072,-0.709089237558,-0.959249055741,-0.520106314628,-1.20753289123,-0.768348226006]\n",
      "-1.38957572958\n",
      "1000\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "print cvmodel.bestModel.coefficients\n",
    "print cvmodel.bestModel.intercept\n",
    "print cvmodel.bestModel._java_obj.getMaxIter()\n",
    "print cvmodel.bestModel._java_obj.getRegParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9088674800439503"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BinaryClassificationEvaluator().evaluate(cvmodel.bestModel.transform(adultvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7691322262239652"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BinaryClassificationEvaluator().setMetricName(\"areaUnderPR\").evaluate(cvmodel.bestModel.transform(adultvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
