{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itPostsRows = sc.textFile(\"../Data/Italian_Stack_Exchange/italianPosts.csv\")\n",
    "itPostsSplit = itPostsRows.map(lambda x: x.split(\"~\"))\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "def toIntSafe(inval):\n",
    "  try:\n",
    "    return int(inval)\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def toTimeSafe(inval):\n",
    "  try:\n",
    "    return datetime.strptime(inval, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def toLongSafe(inval):\n",
    "  try:\n",
    "    return long(inval)\n",
    "  except ValueError:\n",
    "    return None\n",
    "    \n",
    "def stringToPost(row):\n",
    "  r = row.encode('utf8').split(\"~\")\n",
    "  return Row(\n",
    "    toIntSafe(r[0]),\n",
    "    toTimeSafe(r[1]),\n",
    "    toIntSafe(r[2]),\n",
    "    r[3],\n",
    "    toIntSafe(r[4]),\n",
    "    toTimeSafe(r[5]),\n",
    "    toIntSafe(r[6]),\n",
    "    toIntSafe(r[7]),\n",
    "    r[8],\n",
    "    toIntSafe(r[9]),\n",
    "    toLongSafe(r[10]),\n",
    "    toLongSafe(r[11]),\n",
    "    long(r[12]))\n",
    "from pyspark.sql.types import *\n",
    "postSchema = StructType([\n",
    "  StructField(\"commentCount\", IntegerType(), True),\n",
    "  StructField(\"lastActivityDate\", TimestampType(), True),\n",
    "  StructField(\"ownerUserId\", LongType(), True),\n",
    "  StructField(\"body\", StringType(), True),\n",
    "  StructField(\"score\", IntegerType(), True),\n",
    "  StructField(\"creationDate\", TimestampType(), True),\n",
    "  StructField(\"viewCount\", IntegerType(), True),\n",
    "  StructField(\"title\", StringType(), True),\n",
    "  StructField(\"tags\", StringType(), True),\n",
    "  StructField(\"answerCount\", IntegerType(), True),\n",
    "  StructField(\"acceptedAnswerId\", LongType(), True),\n",
    "  StructField(\"postTypeId\", LongType(), True),\n",
    "  StructField(\"id\", LongType(), False)\n",
    "  ])\n",
    "rowRDD = itPostsRows.map(lambda x: stringToPost(x))\n",
    "itPostsDFStruct = sqlContext.createDataFrame(rowRDD, postSchema)\n",
    "\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+--------------------+----+\n",
      "|ownerUserId|  id|        creationDate|prev|\n",
      "+-----------+----+--------------------+----+\n",
      "|          4|1637|2014-01-24 06:51:...|null|\n",
      "|          8|   1|2013-11-05 20:22:...|null|\n",
      "|          8| 112|2013-11-08 13:14:...|   1|\n",
      "|          8|1192|2013-11-11 21:01:...| 112|\n",
      "|          8|1276|2013-11-15 16:09:...|1192|\n",
      "|          8|1321|2013-11-20 16:42:...|1276|\n",
      "|          8|1365|2013-11-23 09:09:...|1321|\n",
      "|         12|  11|2013-11-05 21:30:...|null|\n",
      "|         12|  17|2013-11-05 22:17:...|  11|\n",
      "|         12|  18|2013-11-05 22:34:...|  17|\n",
      "|         12|  19|2013-11-05 22:38:...|  18|\n",
      "|         12|  63|2013-11-06 17:54:...|  19|\n",
      "|         12|  65|2013-11-06 18:07:...|  63|\n",
      "|         12|  69|2013-11-06 19:41:...|  65|\n",
      "|         12|  70|2013-11-06 20:35:...|  69|\n",
      "|         12|  89|2013-11-07 19:22:...|  70|\n",
      "|         12|  94|2013-11-07 20:42:...|  89|\n",
      "|         12| 107|2013-11-08 08:27:...|  94|\n",
      "|         12| 122|2013-11-08 20:55:...| 107|\n",
      "|         12|1141|2013-11-09 20:50:...| 122|\n",
      "+-----------+----+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "postsDf = itPostsDFStruct\n",
    "winDf =postsDf.filter(postsDf.postTypeId == 1).select(postsDf.ownerUserId, postsDf.id, postsDf.creationDate, lag(postsDf.id, 1).over(Window.partitionBy(postsDf.ownerUserId).orderBy(postsDf.creationDate)).alias(\"prev\")).orderBy(postsDf.ownerUserId, postsDf.id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+--------------------+----+----+\n",
      "|ownerUserId|  id|        creationDate|prev|next|\n",
      "+-----------+----+--------------------+----+----+\n",
      "|          4|1637|2014-01-24 06:51:...|null|null|\n",
      "|          8|   1|2013-11-05 20:22:...|null| 112|\n",
      "|          8| 112|2013-11-08 13:14:...|   1|1192|\n",
      "|          8|1192|2013-11-11 21:01:...| 112|1276|\n",
      "|          8|1276|2013-11-15 16:09:...|1192|1321|\n",
      "|          8|1321|2013-11-20 16:42:...|1276|1365|\n",
      "|          8|1365|2013-11-23 09:09:...|1321|null|\n",
      "|         12|  11|2013-11-05 21:30:...|null|  17|\n",
      "|         12|  17|2013-11-05 22:17:...|  11|  18|\n",
      "|         12|  18|2013-11-05 22:34:...|  17|  19|\n",
      "|         12|  19|2013-11-05 22:38:...|  18|  63|\n",
      "|         12|  63|2013-11-06 17:54:...|  19|  65|\n",
      "|         12|  65|2013-11-06 18:07:...|  63|  69|\n",
      "|         12|  69|2013-11-06 19:41:...|  65|  70|\n",
      "|         12|  70|2013-11-06 20:35:...|  69|  89|\n",
      "|         12|  89|2013-11-07 19:22:...|  70|  94|\n",
      "|         12|  94|2013-11-07 20:42:...|  89| 107|\n",
      "|         12| 107|2013-11-08 08:27:...|  94| 122|\n",
      "|         12| 122|2013-11-08 20:55:...| 107|1141|\n",
      "|         12|1141|2013-11-09 20:50:...| 122|1142|\n",
      "+-----------+----+--------------------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "postsDf = itPostsDFStruct\n",
    "winDf =postsDf.filter(postsDf.postTypeId == 1).select(postsDf.ownerUserId, postsDf.id, postsDf.creationDate, lag(postsDf.id, 1).over(Window.partitionBy(postsDf.ownerUserId).orderBy(postsDf.creationDate)).alias(\"prev\"), lead(postsDf.id, 1).over(Window.partitionBy(postsDf.ownerUserId).orderBy(postsDf.creationDate)).alias(\"next\")).orderBy(postsDf.ownerUserId, postsDf.id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
